{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TinyML Non-Contact Heart Rate Monitor (rPPG Pipeline)\n",
        "\n",
        "**Project:** Remote Photoplethysmography (rPPG) on Edge Hardware.\n",
        "**Author:** [Your Name]\n",
        "\n",
        "### **Overview**\n",
        "This notebook implements a computer vision pipeline to detect human heart rate from a standard video feed.\n",
        "1.  **Ingest:** Reads a video file and extracts the **Green Channel** signal from the forehead (Region of Interest).\n",
        "2.  **Process:** Applies Digital Signal Processing (DSP) techniques (Butterworth Bandpass Filter) to isolate the pulse.\n",
        "3.  **Analyze:** Calculates Heart Rate (BPM) using a robust Median Inter-Beat Interval (IBI) algorithm.\n",
        "4.  **Deploy:** Automatically generates valid **C++ Firmware Code** to run this exact logic on an ESP32 microcontroller."
      ],
      "metadata": {
        "id": "eVogI9v49pxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt, find_peaks\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# --- Global Configuration ---\n",
        "FS = 30.0          # Video Frame Rate (Assumed 30fps for standard cameras)\n",
        "LOW_BPM = 42       # Lowest realistic heart rate to detect\n",
        "HIGH_BPM = 180     # Highest realistic heart rate to detect\n",
        "DATA_SLICE = 300   # How many frames to export to C++ (e.g., 300 frames = 10 seconds)\n",
        "\n",
        "print(\"âœ… Libraries imported and configuration set.\")"
      ],
      "metadata": {
        "id": "4WWmA2Ds-JJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Core Processing Functions**\n",
        "Here we define the logic to extract the signal.\n",
        "* **Region of Interest (ROI):** We automatically target the center 20% of the frame. This usually captures the forehead or cheek, which are rich in capillary blood flow.\n",
        "* **Green Channel:** We extract only the Green color channel because hemoglobin absorbs green light most strongly, providing the best Signal-to-Noise Ratio (SNR) for pulse detection."
      ],
      "metadata": {
        "id": "3c4RsKKU-Nlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_roi_rect(frame):\n",
        "    \"\"\"Calculates the coordinates of the ROI box, shifted up to the forehead.\"\"\"\n",
        "    h, w, _ = frame.shape\n",
        "    # Box size is 20% of width and height\n",
        "    box_w, box_h = w // 5, h // 5\n",
        "\n",
        "    # 1. Calculate the dead-center position first\n",
        "    center_x, center_y = w // 2, h // 2\n",
        "\n",
        "    # 2. Calculate standard center box start\n",
        "    start_x = center_x - (box_w // 2)\n",
        "    original_start_y = center_y - (box_h // 2)\n",
        "\n",
        "    # 3. Shift UP by exactly one box height to hit the forehead\n",
        "    start_y = original_start_y - box_h\n",
        "\n",
        "    # 4. Safety check: ensure we don't go off the top edge\n",
        "    if start_y < 0: start_y = 0\n",
        "\n",
        "    return start_x, start_y, box_w, box_h\n",
        "\n",
        "def get_roi_avg(frame):\n",
        "    \"\"\"Extracts Green Channel average from the target ROI.\"\"\"\n",
        "    start_x, start_y, box_w, box_h = get_roi_rect(frame)\n",
        "    # Slice the image to get only the ROI\n",
        "    roi = frame[start_y:start_y+box_h, start_x:start_x+box_w]\n",
        "    # Index 1 is Green in BGR\n",
        "    return np.mean(roi[:, :, 1])\n",
        "\n",
        "def process_video_file(filename):\n",
        "    \"\"\"Reads video, extracts signal, and grabs CROPPED example frames.\"\"\"\n",
        "    cap = cv2.VideoCapture(filename)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Video capture could not open.\")\n",
        "        return np.array([]), []\n",
        "\n",
        "    raw_values = []\n",
        "    example_frames = []\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Grab Start, Middle, and End frames\n",
        "    indices_to_grab = [0, frame_count // 2, frame_count - 1]\n",
        "\n",
        "    current_frame = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        # 1. Get Signal Data (uses the new forehead coordinates)\n",
        "        raw_values.append(get_roi_avg(frame))\n",
        "\n",
        "        # 2. Grab Cropped Example Frames\n",
        "        if current_frame in indices_to_grab:\n",
        "            # Convert BGR to RGB for correct plotting\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            # Get ROI coordinates\n",
        "            sx, sy, bw, bh = get_roi_rect(frame)\n",
        "            # CROP the frame to contain only the ROI\n",
        "            cropped_roi = rgb_frame[sy:sy+bh, sx:sx+bw]\n",
        "            example_frames.append(cropped_roi)\n",
        "\n",
        "        current_frame += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    return np.array(raw_values), example_frames\n",
        "\n",
        "print(\"âœ… Processing functions updated: Forehead target selected.\")"
      ],
      "metadata": {
        "id": "oFI-gkGV-Pjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Data Loading**\n",
        "This block checks if you have already uploaded a video.\n",
        "* If a video exists, it uses it (allowing for rapid iteration).\n",
        "* If no video exists, it prompts you to upload one.\n",
        "* *Note: Keep videos under 60 seconds for faster processing.*"
      ],
      "metadata": {
        "id": "_kX5cgjl-RhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for existing video files\n",
        "video_extensions = ('.mp4', '.mov', '.avi')\n",
        "existing_files = [f for f in os.listdir('.') if f.lower().endswith(video_extensions)]\n",
        "filename = \"\"\n",
        "\n",
        "if existing_files:\n",
        "    filename = existing_files[0]\n",
        "    print(f\"âœ… Found existing video: '{filename}'\")\n",
        "    print(\"   (Skipping upload. To use a new video, delete this file from the folder icon on the left.)\")\n",
        "else:\n",
        "    print(\"ðŸ“‚ Step 1: Upload your video file.\")\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        filename = list(uploaded.keys())[0]\n",
        "    else:\n",
        "        print(\"âŒ No file uploaded.\")\n",
        "\n",
        "# Run extraction if file exists\n",
        "if filename:\n",
        "    print(f\"\\nProcessing {filename}...\")\n",
        "    data, example_frames = process_video_file(filename)\n",
        "    if len(data) == 0:\n",
        "        print(\"âŒ Error: Could not extract data from video.\")\n",
        "    else:\n",
        "        print(f\"âœ… Extracted {len(data)} frames of signal data.\")"
      ],
      "metadata": {
        "id": "0jTbSxmo-TVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Visual Verification**\n",
        "Before analyzing the signal, we must verify the computer is looking at the correct location. Below are three frames from your video showing the **Region of Interest (Green Box)**.\n",
        "* *Check:* Ensure the box is on skin (forehead/cheek) and not drifting off-screen."
      ],
      "metadata": {
        "id": "HLsp5AOU-VNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if len(example_frames) > 0:\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    titles = ['Start Frame', 'Middle Frame', 'End Frame']\n",
        "    for i, frame in enumerate(example_frames):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        plt.imshow(frame)\n",
        "        plt.title(titles[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No frames to display.\")"
      ],
      "metadata": {
        "id": "WITqYEU0-W1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Digital Signal Processing (DSP)**\n",
        "Raw video data is noisy. We apply a pipeline to recover the heartbeat:\n",
        "1.  **Detrending:** Removing the baseline drift caused by breathing or lighting changes.\n",
        "2.  **Bandpass Filter:** We use a **2nd-order Butterworth filter** to keep only frequencies between **0.7Hz** (42 BPM) and **3.0Hz** (180 BPM).\n",
        "3.  **Median IBI Calculation:** Instead of averaging beats over time (which is sensitive to silence at the start), we calculate the time interval between every pair of beats and take the median."
      ],
      "metadata": {
        "id": "gD4Vf92K-YaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if len(data) > 0:\n",
        "    # 1. Detrend (Center at 0)\n",
        "    data = data - np.mean(data)\n",
        "\n",
        "    # 2. Design Bandpass Filter\n",
        "    nyquist = 0.5 * FS\n",
        "    low = (LOW_BPM / 60.0) / nyquist\n",
        "    high = (HIGH_BPM / 60.0) / nyquist\n",
        "    b, a = butter(2, [low, high], btype='band')\n",
        "\n",
        "    # 3. Apply Filter\n",
        "    filtered_data = filtfilt(b, a, data)\n",
        "\n",
        "    # 4. Peak Detection\n",
        "    # We enforce a minimum distance of 0.6 seconds (approx 100 BPM limit) to avoid double-counting noise\n",
        "    min_dist = int(FS * 0.6)\n",
        "    peaks, _ = find_peaks(filtered_data, distance=min_dist)\n",
        "\n",
        "    # 5. Calculate BPM (Median Inter-Beat Interval Method)\n",
        "    bpm = 0.0\n",
        "    if len(peaks) > 1:\n",
        "        intervals = np.diff(peaks) / FS\n",
        "        bpm_instant = 60.0 / intervals\n",
        "        bpm = np.median(bpm_instant)\n",
        "\n",
        "    print(f\"âœ… DSP Complete. Estimated Heart Rate: {bpm:.2f} BPM\")"
      ],
      "metadata": {
        "id": "AM8O-b5b-Z-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Results Visualization**\n",
        "* **Top Graph (Green):** The raw, noisy signal extracted from the camera. Note the drift and jitter.\n",
        "* **Bottom Graph (Blue):** The filtered pulse wave. The orange 'x' markers indicate detected heartbeats."
      ],
      "metadata": {
        "id": "wwLTpF7d-b3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if len(data) > 0:\n",
        "    time_axis = np.arange(len(data)) / FS\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot Raw\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(time_axis, data, color='green', alpha=0.5, label='Raw Signal')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(f\"Analysis Result: {bpm:.1f} BPM\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot Filtered\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(time_axis, filtered_data, color='blue', label='Filtered Pulse')\n",
        "    plt.scatter(time_axis[peaks], filtered_data[peaks], color='orange', marker='x', s=50, label='Detected Beats')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2yhsQs18-fJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Automated Firmware Generation (MLOps)**\n",
        "To deploy this to an **ESP32** or **Arduino**, we need C++ code. Manually copying array values is error-prone.\n",
        "The block below automatically formats our raw Python data and our Filter Coefficients into valid C++ code.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Run the cell below.\n",
        "2.  Copy the output block.\n",
        "3.  Paste it into the [Wokwi Simulator](https://wokwi.com/projects/new/esp32) to verify the embedded performance."
      ],
      "metadata": {
        "id": "GV8YS8Bl-g0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if len(data) > 0:\n",
        "    # Format data array for C++ (Take a slice to save memory)\n",
        "    cpp_data = data[:DATA_SLICE]\n",
        "    data_str = \", \".join([f\"{x:.2f}\" for x in cpp_data])\n",
        "\n",
        "    cpp_template = f\"\"\"#include <Arduino.h>\n",
        "\n",
        "/* rPPG Heart Rate Filter (Auto-Generated)\n",
        "   Source Video: {filename}\n",
        "   Python Estimated BPM: {bpm:.1f}\n",
        "*/\n",
        "\n",
        "// --- RAW DATA FROM PYTHON ({len(cpp_data)} frames) ---\n",
        "const int DATA_LEN = {len(cpp_data)};\n",
        "float raw_data[] = {{\n",
        "  {data_str}\n",
        "}};\n",
        "\n",
        "// --- FILTER COEFFICIENTS (Butterworth Bandpass) ---\n",
        "// Generated automatically from Python scipy.signal\n",
        "float b0 = {b[0]:.6f}, b1 = {b[1]:.6f}, b2 = {b[2]:.6f}, b3 = {b[3]:.6f}, b4 = {b[4]:.6f};\n",
        "float a0 = {a[0]:.6f}, a1 = {a[1]:.6f}, a2 = {a[2]:.6f}, a3 = {a[3]:.6f}, a4 = {a[4]:.6f};\n",
        "\n",
        "// History Buffers\n",
        "float x[5] = {{0,0,0,0,0}};\n",
        "float y[5] = {{0,0,0,0,0}};\n",
        "\n",
        "void setup() {{\n",
        "  Serial.begin(115200);\n",
        "  Serial.println(\"Starting Simulation...\");\n",
        "  Serial.println(\"Raw_Signal,Filtered_Pulse\"); // Plotter Legend\n",
        "}}\n",
        "\n",
        "void loop() {{\n",
        "  for(int i=0; i < DATA_LEN; i++) {{\n",
        "\n",
        "    float input = raw_data[i];\n",
        "\n",
        "    // Shift History\n",
        "    for(int k=4; k>0; k--) {{\n",
        "      x[k] = x[k-1];\n",
        "      y[k] = y[k-1];\n",
        "    }}\n",
        "    x[0] = input;\n",
        "\n",
        "    // Difference Equation (Direct Form II Transposed / Standard)\n",
        "    float output = (b0*x[0] + b1*x[1] + b2*x[2] + b3*x[3] + b4*x[4])\n",
        "                 - (a1*y[1] + a2*y[2] + a3*y[3] + a4*y[4]);\n",
        "\n",
        "    y[0] = output;\n",
        "\n",
        "    // Print for Serial Plotter\n",
        "    Serial.print(input);\n",
        "    Serial.print(\",\");\n",
        "    Serial.println(output * 50);\n",
        "\n",
        "    delay(33); // Simulate 30 FPS\n",
        "  }}\n",
        "\n",
        "  // Stop after one pass\n",
        "  while(1) {{ delay(10); }}\n",
        "}}\n",
        "\"\"\"\n",
        "    print(cpp_template)"
      ],
      "metadata": {
        "id": "W9XNTTb5-iex"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}